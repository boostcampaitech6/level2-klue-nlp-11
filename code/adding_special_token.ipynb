{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행 단계: 모든 파일은 코드 확인 및 수정 후 실행할 것\n",
    "# 1. data_aug.ipynb -> 2. adding_special_token.ipynb -> 3. separate_dev_data.ipynb -> \n",
    "# 4. config.yaml 수정 -> 5. train.py -> 6. inference.py -> 7. ensemble.ipynb(4~7 k번 반복)\n",
    "\n",
    "# 노트북 파일을 dataset 폴더에서 code 폴더로 옮겼기에, 데이터 경로를 바꿔야 함\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"./train/train_aug_copy.csv\")\n",
    "test_data = pd.read_csv(\"./test/test_data.csv\")\n",
    "\n",
    "MARKERS = dict(\n",
    "    subject_start_marker=\"<SUB>\",\n",
    "    subject_end_marker  =\"</SUB>\",\n",
    "    object_start_marker =\"<OBJ>\",\n",
    "    object_end_marker   =\"</OBJ>\",\n",
    ")\n",
    "TYPE_MARKERS = dict(\n",
    "    subject_start_per_marker=\"<S:PER>\",\n",
    "    subject_start_org_marker=\"<S:ORG>\",\n",
    "    subject_start_loc_marker=\"<S:LOC>\",\n",
    "    subject_start_poh_marker=\"<S:POH>\",\n",
    "    subject_start_dat_marker=\"<S:DAT>\",\n",
    "    subject_start_noh_marker=\"<S:NOH>\",\n",
    "    subject_end_per_marker =\"</S:PER>\",\n",
    "    subject_end_org_marker =\"</S:ORG>\",\n",
    "    subject_end_loc_marker=\"</S:LOC>\",\n",
    "    subject_end_poh_marker=\"</S:POH>\",\n",
    "    subject_end_dat_marker=\"</S:DAT>\",\n",
    "    subject_end_noh_marker=\"</S:NOH>\",\n",
    "    object_start_per_marker=\"<O:PER>\",\n",
    "    object_start_org_marker=\"<O:ORG>\",\n",
    "    object_start_loc_marker=\"<O:LOC>\",\n",
    "    object_start_dat_marker=\"<O:DAT>\",\n",
    "    object_start_poh_marker=\"<O:POH>\",\n",
    "    object_start_noh_marker=\"<O:NOH>\",\n",
    "    object_end_per_marker =\"</O:PER>\",\n",
    "    object_end_org_marker =\"</O:ORG>\",\n",
    "    object_end_loc_marker =\"</O:LOC>\",\n",
    "    object_end_dat_marker =\"</O:DAT>\",\n",
    "    object_end_poh_marker =\"</O:POH>\",\n",
    "    object_end_noh_marker =\"</O:NOH>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity 정보 분리 - train\n",
    "# 분리된 entity를 저장할 리스트\n",
    "subject_word = []\n",
    "subject_start = []\n",
    "subject_end = []\n",
    "subject_type = []\n",
    "\n",
    "object_word = []\n",
    "object_start = []\n",
    "object_end = []\n",
    "object_type = []\n",
    "\n",
    "# train data에 데이터 추가\n",
    "from ast import literal_eval\n",
    "for idx, row in train_data.iterrows():\n",
    "    # print(idx)\n",
    "    sub_data = literal_eval(row['subject_entity']) # type==dict\n",
    "    obj_data = literal_eval(row['object_entity'])\n",
    "    \n",
    "    subject_word.append(sub_data['word'])\n",
    "    subject_start.append(int(sub_data['start_idx']))\n",
    "    subject_end.append(int(sub_data['end_idx']))\n",
    "    subject_type.append(sub_data['type'])\n",
    "    \n",
    "    object_word.append(obj_data['word'])\n",
    "    object_start.append(int(obj_data['start_idx']))\n",
    "    object_end.append(int(obj_data['end_idx']))\n",
    "    object_type.append(obj_data['type'])\n",
    "\n",
    "# 데이터 추가\n",
    "train_data['subject_word'] = subject_word\n",
    "train_data['subject_start'] = subject_start\n",
    "train_data['subject_end'] = subject_end\n",
    "train_data['subject_type'] = subject_type\n",
    "\n",
    "train_data['object_word'] = object_word\n",
    "train_data['object_start'] = object_start\n",
    "train_data['object_end'] = object_end\n",
    "train_data['object_type'] = object_type\n",
    "\n",
    "# 미사용 컬럼 삭제\n",
    "#train_data = train_data.drop(['subject_entity', 'object_entity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity 정보 분리 - test\n",
    "# 분리된 entity를 저장할 리스트\n",
    "subject_word = []\n",
    "subject_start = []\n",
    "subject_end = []\n",
    "subject_type = []\n",
    "\n",
    "object_word = []\n",
    "object_start = []\n",
    "object_end = []\n",
    "object_type = []\n",
    "\n",
    "# train data에 데이터 추가\n",
    "from ast import literal_eval\n",
    "for idx, row in test_data.iterrows():\n",
    "    # print(idx)\n",
    "    sub_data = literal_eval(row['subject_entity']) # type==dict\n",
    "    obj_data = literal_eval(row['object_entity'])\n",
    "    \n",
    "    subject_word.append(sub_data['word'])\n",
    "    subject_start.append(int(sub_data['start_idx']))\n",
    "    subject_end.append(int(sub_data['end_idx']))\n",
    "    subject_type.append(sub_data['type'])\n",
    "    \n",
    "    object_word.append(obj_data['word'])\n",
    "    object_start.append(int(obj_data['start_idx']))\n",
    "    object_end.append(int(obj_data['end_idx']))\n",
    "    object_type.append(obj_data['type'])\n",
    "\n",
    "# 데이터 추가\n",
    "test_data['subject_word'] = subject_word\n",
    "test_data['subject_start'] = subject_start\n",
    "test_data['subject_end'] = subject_end\n",
    "test_data['subject_type'] = subject_type\n",
    "\n",
    "test_data['object_word'] = object_word\n",
    "test_data['object_start'] = object_start\n",
    "test_data['object_end'] = object_end\n",
    "test_data['object_type'] = object_type\n",
    "\n",
    "# 미사용 컬럼 삭제\n",
    "#test_data = test_data.drop(['subject_entity', 'object_entity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_marker(data : pd.Series):\n",
    "    # 예시: 〈Something〉는 <OBJ> 조지 해리슨 </OBJ>이 쓰고 <SUB> 비틀즈 </SUB>가 1969년 앨범 《Abbey Road》에 담은 노래다.\n",
    "    sent = data['sentence']\n",
    "    sbj = data['subject_word']\n",
    "    obj = data['object_word']\n",
    "    sent = sent.replace(sbj, MARKERS['subject_start_marker']+' '+sbj+' '+MARKERS['subject_end_marker'])\n",
    "    sent = sent.replace(obj, MARKERS['object_start_marker']+' '+obj+' '+MARKERS['object_end_marker'])\n",
    "    return sent\n",
    "\n",
    "def typed_entity_marker(data : pd.Series):\n",
    "    # 예시: 〈Something〉는 <O:PER> 조지 해리슨 </O:PER>이 쓰고 <S:ORG> 비틀즈 </S:ORG>가 1969년 앨범 《Abbey Road》에 담은 노래다.\n",
    "    sent = data['sentence']\n",
    "    sbj = data['subject_word']\n",
    "    sbj_start_type_mark = TYPE_MARKERS[f\"subject_start_{data['subject_type'].lower()}_marker\"]\n",
    "    sbj_end_type_mark = TYPE_MARKERS[f\"subject_end_{data['subject_type'].lower()}_marker\"]\n",
    "    obj = data['object_word']\n",
    "    obj_start_type_mark = TYPE_MARKERS[f\"object_start_{data['object_type'].lower()}_marker\"]\n",
    "    obj_end_type_mark = TYPE_MARKERS[f\"object_end_{data['object_type'].lower()}_marker\"]\n",
    "    sent = sent.replace(sbj, sbj_start_type_mark+' '+sbj+' '+sbj_end_type_mark)\n",
    "    sent = sent.replace(obj, obj_start_type_mark+' '+obj+' '+obj_end_type_mark)\n",
    "    return sent\n",
    "\n",
    "def typed_entity_marker_punc(data : pd.Series):\n",
    "    # 예시: 〈Something〉는 # * PER * 조지 해리슨 # 이 쓰고 @ * ORG * 비틀즈 @ 가 1969년 앨범 《Abbey Road》에 담은 노래다.\n",
    "    sent = data['sentence']\n",
    "    sbj = data['subject_word']\n",
    "    sbj_type = data['subject_type']\n",
    "    obj = data['object_word']\n",
    "    obj_type = data['object_type']\n",
    "    sent = sent.replace(sbj, '@'+f' * {sbj_type} * '+sbj+' @ ')\n",
    "    sent = sent.replace(obj, '#'+f' * {obj_type} * '+obj+' # ')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터 전처리 후 저장\n",
    "entity_marker_list = []\n",
    "typed_entity_marker_list = []\n",
    "typed_entity_marker_punc_list = []\n",
    "for i in range(len(train_data)):\n",
    "    entity_marker_list.append(entity_marker(train_data.iloc[i]))\n",
    "    typed_entity_marker_list.append(typed_entity_marker(train_data.iloc[i]))\n",
    "    typed_entity_marker_punc_list.append(typed_entity_marker_punc(train_data.iloc[i]))\n",
    "    \n",
    "train_data['sentence'] = entity_marker_list\n",
    "train_data.to_csv(\"./train/train_aug_copy_entity_marker.csv\")\n",
    "train_data['sentence'] = typed_entity_marker_list\n",
    "train_data.to_csv(\"./train/train_aug_copy_typed_entity_marker.csv\")\n",
    "train_data['sentence'] = typed_entity_marker_punc_list\n",
    "train_data.to_csv(\"./train/train_aug_copy_typed_entity_marker_punc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터 전처리 후 저장\n",
    "entity_marker_list = []\n",
    "typed_entity_marker_list = []\n",
    "typed_entity_marker_punc_list = []\n",
    "for i in range(len(test_data)):\n",
    "    entity_marker_list.append(entity_marker(test_data.iloc[i]))\n",
    "    typed_entity_marker_list.append(typed_entity_marker(test_data.iloc[i]))\n",
    "    typed_entity_marker_punc_list.append(typed_entity_marker_punc(test_data.iloc[i]))\n",
    "    \n",
    "test_data['sentence'] = entity_marker_list\n",
    "test_data.to_csv(\"./test/test_entity_marker.csv\")\n",
    "test_data['sentence'] = typed_entity_marker_list\n",
    "test_data.to_csv(\"./test/test_typed_entity_marker.csv\")\n",
    "test_data['sentence'] = typed_entity_marker_punc_list\n",
    "test_data.to_csv(\"./test/test_typed_entity_marker_punc.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
