{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, get_cosine_schedule_with_warmup\n",
    "from load_data_copy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "       'org:product', 'per:title', 'org:alternate_names',\n",
    "       'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "       'org:number_of_employees/members', 'per:children',\n",
    "       'per:place_of_residence', 'per:alternate_names',\n",
    "       'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "       'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "       'org:member_of', 'per:parents', 'org:dissolved',\n",
    "       'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "       'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "       'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  \"\"\" validationÏùÑ ÏúÑÌïú metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1(preds, labels)\n",
    "  auprc = klue_re_auprc(probs, labels)\n",
    "  acc = accuracy_score(labels, preds) # Î¶¨ÎçîÎ≥¥Îìú ÌèâÍ∞ÄÏóêÎäî Ìè¨Ìï®ÎêòÏßÄ ÏïäÏäµÎãàÎã§.\n",
    "\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "\n",
    "def label_to_num(label):\n",
    "  num_label = []\n",
    "  with open('dict_label_to_num.pkl', 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, weight=None,\n",
    "#                  gamma=2.0, reduction='mean'):\n",
    "#         nn.Module.__init__(self)\n",
    "#         self.weight = weight\n",
    "#         self.gamma = gamma\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, input_tensor, target_tensor):\n",
    "#         log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "#         prob = torch.exp(log_prob)\n",
    "#         return F.nll_loss(\n",
    "#             ((1 - prob) ** self.gamma) * log_prob,\n",
    "#             target_tensor,\n",
    "#             weight=self.weight,\n",
    "#             reduction=self.reduction\n",
    "#         )\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True, balancing=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.balancing_alpha = { # key means label that changed to number\n",
    "             0: 0.75, #\n",
    "             1: 0.75, #\n",
    "             6: 0.75, #\n",
    "             4: 0.75, #\n",
    "            20: 0.75, #\n",
    "             5: 0.75, #\n",
    "            15: 0.75, #\n",
    "             7: 0.75, #\n",
    "            25: 0.75, #\n",
    "            12: 0.75, #\n",
    "            17: 0.75, #\n",
    "            14: 0.25, #\n",
    "            21: 0.25, #\n",
    "            18: 0.25, #\n",
    "             2: 0.25, #\n",
    "            24: 0.25, #\n",
    "             3: 0.25, #\n",
    "            10: 0.25, #\n",
    "            11: 0.25, #\n",
    "            13: 0.25, #\n",
    "            26: 0.25, #\n",
    "            28: 0.25, #\n",
    "             8: 0.25, #\n",
    "            16: 0.25, #\n",
    "            19: 0.25, #\n",
    "            29: 0.25, #\n",
    "            23: 0.25, #\n",
    "            22: 0.25, #\n",
    "             9: 0.25, #\n",
    "            27: 0.25 # \n",
    "            }\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "        self.balancing = balancing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        if self.balancing : \n",
    "            targets_copy = targets.tolist()\n",
    "            # print(\"targets\", targets_copy)\n",
    "            alpha_t = torch.Tensor([self.balancing_alpha[k] for k in targets_copy]).view(-1, 1).to('cuda')\n",
    "            F_loss = alpha_t * (1-pt)**self.gamma * ce_loss\n",
    "        else:\n",
    "            F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")  \n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = FocalLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.get(\"labels\")\n",
    "#         # forward pass\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get(\"logits\")\n",
    "#         # compute custom loss (suppose one has 3 labels with different weights)\n",
    "#         loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n",
    "#         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # load model and tokenizer\n",
    "    # MODEL_NAME = \"bert-base-uncased\"\n",
    "    # MODEL_NAME = \"klue/bert-base\"\n",
    "    MODEL_NAME = \"klue/roberta-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, additional_special_tokens=['#', '@'])\n",
    "    \n",
    "    # load dataset\n",
    "    train_dataset = load_data('../../dataAugmentation/entity_split_dup_del.csv')\n",
    "    dev_dataset = load_data('../../dataAugmentation/dev_split.csv') # validationÏö© Îç∞Ïù¥ÌÑ∞Îäî Îî∞Î°ú ÎßåÎìúÏÖîÏïº Ìï©ÎãàÎã§.\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "    # subj_start_id = tokenizer.convert_tokens_to_ids([\"<S:PER>\", \"<S:ORG>\"])\n",
    "    # subj_end_id = tokenizer.convert_tokens_to_ids([\"</S:PER>\", \"</S:ORG>\"])\n",
    "    # obj_start_id = tokenizer.convert_tokens_to_ids([\"<O:PER>\", \"<O:ORG>\", \"<O:LOC>\", \"<O:DAT>\", \"<O:POH>\", \"<O:NOH>\"])\n",
    "    # obj_end_id = tokenizer.convert_tokens_to_ids([\"</O:PER>\", \"</O:ORG>\", \"</O:LOC>\", \"</O:DAT>\", \"</O:POH>\", \"</O:NOH>\"])\n",
    "    # start_id = subj_start_id+obj_start_id\n",
    "    # end_id   = subj_end_id+obj_end_id\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "    train_ent_pos_emb = get_entity_position_embedding(tokenizer, tokenized_train['input_ids'])\n",
    "    print(len(tokenized_train['input_ids'][0]))\n",
    "    for i in train_ent_pos_emb:\n",
    "        if len(i) == 4:continue\n",
    "        else:\n",
    "            print(\"error\")\n",
    "    tokenized_train['entity_ids'] = making_entity_pos_emb(train_ent_pos_emb)\n",
    "    # entity_ids = entity_ids_maker(train_dataset, tokenizer)\n",
    "    # tokenized_train['entity_ids'] = entity_ids\n",
    "    \n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "    dev_ent_pos_emb = get_entity_position_embedding(tokenizer, tokenized_dev['input_ids'])\n",
    "    tokenized_dev['entity_ids'] = making_entity_pos_emb(dev_ent_pos_emb)\n",
    "    # entity_ids = entity_ids_maker(dev_dataset, tokenizer)\n",
    "    # tokenized_dev['entity_ids'] = entity_ids\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(device)\n",
    "    # setting model hyperparameter\n",
    "    model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 30\n",
    "    model_config.classifier_dropout = 0.1\n",
    "    \n",
    "    model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(model.config)\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "    \n",
    "    # ÏÇ¨Ïö©Ìïú option Ïô∏ÏóêÎèÑ Îã§ÏñëÌïú optionÎì§Ïù¥ ÏûàÏäµÎãàÎã§.\n",
    "    # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments Ï∞∏Í≥†Ìï¥Ï£ºÏÑ∏Ïöî.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        save_total_limit=5,              # number of total save model.\n",
    "        save_steps=500,                 # model saving step.\n",
    "        num_train_epochs=5,              # total number of training epochs\n",
    "        learning_rate=3e-5,               # learning_rate\n",
    "        per_device_train_batch_size=48,  # batch size per device during training\n",
    "        per_device_eval_batch_size=48,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=100,              # log saving step.\n",
    "        evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                    # `no`: No evaluation during training.\n",
    "                                    # `steps`: Evaluate every `eval_steps`.\n",
    "                                    # `epoch`: Evaluate every end of epoch.\n",
    "        fp16=True,                                    \n",
    "        eval_steps = 500,            # evaluation step.\n",
    "        load_best_model_at_end = True \n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)  # Adjust the learning rate as needed\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=RE_train_dataset,         # training dataset\n",
    "        eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "        compute_metrics=compute_metrics,         # define metrics function\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "    # trainer.optimizer = torch.optim.AdamW\n",
    "    # trainer.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "    \n",
    "    # train model\n",
    "    trainer.train()\n",
    "    model.save_pretrained('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29122/29122 [00:12<00:00, 2302.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29122it [00:00, 137575.47it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7765/7765 [00:03<00:00, 2523.14it/s]\n",
      "7765it [00:00, 131117.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='3035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/3035 20:27 < 16:19, 1.37 it/s, Epoch 2.78/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro f1 score</th>\n",
       "      <th>Auprc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.751857</td>\n",
       "      <td>63.688946</td>\n",
       "      <td>60.977823</td>\n",
       "      <td>0.753638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.636927</td>\n",
       "      <td>67.235495</td>\n",
       "      <td>73.532584</td>\n",
       "      <td>0.783902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.625216</td>\n",
       "      <td>71.808029</td>\n",
       "      <td>77.247212</td>\n",
       "      <td>0.794591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "  train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
