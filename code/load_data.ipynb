{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils.utils import entity_marker, typed_entity_marker, typed_entity_marker_punc, TYPE_MARKERS, TYPE_MARKERS_PUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "    def __init__(self, pair_dataset, labels):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "    typed_sentence = []\n",
    "    for i, data in dataset.iterrows():\n",
    "        typed_sentence.append(typed_entity_marker_punc(data))\n",
    "        # print(data['sentence'])\n",
    "    out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':typed_sentence, 'subject_entity':dataset['subject_word'], 'object_entity':dataset['object_word'], 'label':dataset['label'],})\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    dataset = preprocessing_dataset(pd_dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_embedding(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    start_id,\n",
    "    end_id\n",
    ") :#-> Dict[str, List[Any]]:\n",
    "    \"\"\" returns entity embeddings \"\"\"\n",
    "    # subj_start_id = tokenizer.convert_tokens_to_ids([\"<S:PER>\", \"<S:ORG>\"])\n",
    "    # subj_end_id = tokenizer.convert_tokens_to_ids([\"</S:PER>\", \"</S:ORG>\"])\n",
    "    # obj_start_id = tokenizer.convert_tokens_to_ids([\"<O:PER>\", \"<O:ORG>\", \"<O:LOC>\", \"<O:DAT>\", \"<O:POH>\", \"<O:NOH>\"])\n",
    "    # obj_end_id = tokenizer.convert_tokens_to_ids([\"</O:PER>\", \"</O:ORG>\", \"</O:LOC>\", \"</O:DAT>\", \"</O:POH>\", \"</O:NOH>\"])\n",
    "\n",
    "    entity_ids = []\n",
    "    is_entity = False\n",
    "    \n",
    "    # start_id = subj_start_id+obj_start_id\n",
    "    # end_id   = subj_end_id+obj_end_id\n",
    "    \n",
    "    for input_id in examples:\n",
    "        if input_id in end_id:\n",
    "            is_entity = False\n",
    "            \n",
    "        entity_id = 1 if is_entity else 0\n",
    "        entity_ids.append(entity_id)\n",
    "        \n",
    "        if input_id in start_id:\n",
    "            is_entity = True\n",
    "    # entity_ids = torch.Tensor(entity_ids)\n",
    "    return entity_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        # Prompting Sentence \n",
    "        temp = ''\n",
    "        temp = e01 + '와(과)' + e02 + '은(는)?' #+'[SEP]'\n",
    "        concat_entity.append(temp)\n",
    "        \n",
    "    # num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\" : list(TYPE_MARKERS_PUNC.values())})\n",
    "    # print(\"We have added\", num_added_toks, \"tokens\")\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "        list(dataset['sentence']),\n",
    "        concat_entity,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=True,\n",
    "        )\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_ids_maker(data, start_id, end_id): # data에는 tokenizer를 거쳐 나온 input_ids가 들어온다\n",
    "    def update_ranges_to_1(start_tokens, end_tokens, maxlen=251):\n",
    "        # print(start_tokens)\n",
    "        # print(end_tokens)\n",
    "        res = []\n",
    "        res += [0] * (start_tokens[0]+1) + [1] * (end_tokens[0]-start_tokens[0]-1)\\\n",
    "        + [0] * (start_tokens[1]-end_tokens[0]+1)\\\n",
    "        + [1] * (end_tokens[1]-start_tokens[1]-1)\\\n",
    "        + [0] * (maxlen-end_tokens[1])\n",
    "        return res\n",
    "    \n",
    "             \n",
    "    entity_ids = []\n",
    "    for ids in tqdm(data):\n",
    "\n",
    "        startidx = []\n",
    "        endidx = []\n",
    "        for i in range(len(ids)):\n",
    "            if ids[i] in start_id:\n",
    "                # print(tokens[i])\n",
    "                startidx.append(i)\n",
    "                \n",
    "            elif ids[i] in end_id:\n",
    "                # print(tokens[i])\n",
    "                endidx.append(i)\n",
    "                \n",
    "        tmp = update_ranges_to_1(startidx, endidx, maxlen=len(ids))\n",
    "        \n",
    "        entity_ids.append(tmp)\n",
    "    entity_ids = torch.Tensor(entity_ids)\n",
    "    return entity_ids # Tensor로 변환하기\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, additional_special_tokens=['#', '@'])\n",
    "train_dataset = load_data('../../dataAugmentation/entity_split.csv')\n",
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "\n",
    "# subj_start_id = tokenizer.convert_tokens_to_ids([\"<S:PER>\", \"<S:ORG>\"])\n",
    "# subj_end_id = tokenizer.convert_tokens_to_ids([\"</S:PER>\", \"</S:ORG>\"])\n",
    "# obj_start_id = tokenizer.convert_tokens_to_ids([\"<O:PER>\", \"<O:ORG>\", \"<O:LOC>\", \"<O:DAT>\", \"<O:POH>\", \"<O:NOH>\"])\n",
    "# obj_end_id = tokenizer.convert_tokens_to_ids([\"</O:PER>\", \"</O:ORG>\", \"</O:LOC>\", \"</O:DAT>\", \"</O:POH>\", \"</O:NOH>\"])\n",
    "# start_id = subj_start_id+obj_start_id\n",
    "# end_id   = subj_end_id+obj_end_id\n",
    "\n",
    "# tokenized_train['entity_ids'] = entity_ids_maker(tokenized_train['input_ids'], start_id, end_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 이 밑은 테스트 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   168, 30985, 14451,  7088,  4586,   169,   793,     7,    14,\n",
       "        21639,    14,  8373, 14113,  2234,     7,  1504,  1363,  2088,    36,\n",
       "           14,    51,  2107,  2341,    14, 29830,    36,   543, 14879,  2440,\n",
       "         6711,   170, 21406, 26713,  2076, 25145,  5749,   171,  1421,   818,\n",
       "         2073,  4388,  2062,    18,     2, 29830,  2522,    12,   604,    13,\n",
       "         8373, 14113,  2234,  2073,    12,   793,    13,    35,     2,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 58.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = []\n",
    "subj_start_id = tokenizer.convert_tokens_to_ids([\"<S:PER>\", \"<S:ORG>\"])\n",
    "subj_end_id = tokenizer.convert_tokens_to_ids([\"</S:PER>\", \"</S:ORG>\"])\n",
    "obj_start_id = tokenizer.convert_tokens_to_ids([\"<O:PER>\", \"<O:ORG>\", \"<O:LOC>\", \"<O:DAT>\", \"<O:POH>\", \"<O:NOH>\"])\n",
    "obj_end_id = tokenizer.convert_tokens_to_ids([\"</O:PER>\", \"</O:ORG>\", \"</O:LOC>\", \"</O:DAT>\", \"</O:POH>\", \"</O:NOH>\"])\n",
    "start_id = subj_start_id+obj_start_id\n",
    "end_id   = subj_end_id+obj_end_id\n",
    "\n",
    "# for ids in tqdm(tokenized_train['input_ids']):\n",
    "#     entity_list.append(get_entity_embedding(ids, tokenizer, start_id, end_id))\n",
    "# entity_list\n",
    "len(entity_ids_maker(tokenized_train['input_ids'][:5], start_id, end_id)[0]) # 256\n",
    "len(tokenized_train['input_ids'][0]) # 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ 가 1969년 앨범 《 Abbey Road 》 에 담은 노래다. [SEP] 비틀즈와 ( 과 ) 조지 해리슨은 ( 는 )? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train['input_ids'][0][26 : 256])\n",
    "# len(tokenized_train['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'entity_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenized_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/venv1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:253\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entity_ids'"
     ]
    }
   ],
   "source": [
    "tokenized_train['entity_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35,\n",
       " 38,\n",
       " 51,\n",
       " 46,\n",
       " 30,\n",
       " 56,\n",
       " 55,\n",
       " 59,\n",
       " 85,\n",
       " 67,\n",
       " 26,\n",
       " 51,\n",
       " 122,\n",
       " 58,\n",
       " 74,\n",
       " 69,\n",
       " 96,\n",
       " 38,\n",
       " 48,\n",
       " 51,\n",
       " 30,\n",
       " 62,\n",
       " 58,\n",
       " 60,\n",
       " 37,\n",
       " 82,\n",
       " 52,\n",
       " 46,\n",
       " 29,\n",
       " 42,\n",
       " 51,\n",
       " 65,\n",
       " 142,\n",
       " 31,\n",
       " 59,\n",
       " 60,\n",
       " 37,\n",
       " 70,\n",
       " 57,\n",
       " 39,\n",
       " 28,\n",
       " 44,\n",
       " 77,\n",
       " 53,\n",
       " 108,\n",
       " 45,\n",
       " 52,\n",
       " 45,\n",
       " 140,\n",
       " 37,\n",
       " 46,\n",
       " 78,\n",
       " 52,\n",
       " 46,\n",
       " 30,\n",
       " 57,\n",
       " 43,\n",
       " 103,\n",
       " 33,\n",
       " 81,\n",
       " 42,\n",
       " 80,\n",
       " 26,\n",
       " 87,\n",
       " 51,\n",
       " 59,\n",
       " 85,\n",
       " 106,\n",
       " 40,\n",
       " 51,\n",
       " 31,\n",
       " 43,\n",
       " 47,\n",
       " 70,\n",
       " 90,\n",
       " 38,\n",
       " 62,\n",
       " 50,\n",
       " 78,\n",
       " 71,\n",
       " 35,\n",
       " 45,\n",
       " 51,\n",
       " 64,\n",
       " 63,\n",
       " 77,\n",
       " 23,\n",
       " 79,\n",
       " 80,\n",
       " 38,\n",
       " 43,\n",
       " 49,\n",
       " 68,\n",
       " 57,\n",
       " 57,\n",
       " 44,\n",
       " 31,\n",
       " 48,\n",
       " 32,\n",
       " 77,\n",
       " 53,\n",
       " 50,\n",
       " 35,\n",
       " 113,\n",
       " 66,\n",
       " 16,\n",
       " 39,\n",
       " 85,\n",
       " 52,\n",
       " 47,\n",
       " 74,\n",
       " 69,\n",
       " 73,\n",
       " 50,\n",
       " 54,\n",
       " 110,\n",
       " 32,\n",
       " 45,\n",
       " 88,\n",
       " 39,\n",
       " 36,\n",
       " 31,\n",
       " 76,\n",
       " 51,\n",
       " 114,\n",
       " 23,\n",
       " 55,\n",
       " 49,\n",
       " 87,\n",
       " 59,\n",
       " 106,\n",
       " 49,\n",
       " 40,\n",
       " 34,\n",
       " 43,\n",
       " 59,\n",
       " 34,\n",
       " 26,\n",
       " 59,\n",
       " 87,\n",
       " 27,\n",
       " 106,\n",
       " 46,\n",
       " 72,\n",
       " 28,\n",
       " 51,\n",
       " 61,\n",
       " 48,\n",
       " 18,\n",
       " 53,\n",
       " 38,\n",
       " 43,\n",
       " 36,\n",
       " 41,\n",
       " 55,\n",
       " 70,\n",
       " 39,\n",
       " 62,\n",
       " 74,\n",
       " 35,\n",
       " 19,\n",
       " 59,\n",
       " 30,\n",
       " 46,\n",
       " 44,\n",
       " 40,\n",
       " 51,\n",
       " 70,\n",
       " 101,\n",
       " 35,\n",
       " 131,\n",
       " 30,\n",
       " 85,\n",
       " 49,\n",
       " 31,\n",
       " 40,\n",
       " 62,\n",
       " 62,\n",
       " 38,\n",
       " 67,\n",
       " 48,\n",
       " 59,\n",
       " 33,\n",
       " 53,\n",
       " 49,\n",
       " 33,\n",
       " 42,\n",
       " 43,\n",
       " 63,\n",
       " 28,\n",
       " 61,\n",
       " 34,\n",
       " 56,\n",
       " 54,\n",
       " 85,\n",
       " 85,\n",
       " 36,\n",
       " 33,\n",
       " 69,\n",
       " 61,\n",
       " 34,\n",
       " 61,\n",
       " 58,\n",
       " 66,\n",
       " 176,\n",
       " 45,\n",
       " 141,\n",
       " 91,\n",
       " 92,\n",
       " 34,\n",
       " 32,\n",
       " 25,\n",
       " 45,\n",
       " 76,\n",
       " 74,\n",
       " 67,\n",
       " 40,\n",
       " 64,\n",
       " 44,\n",
       " 29,\n",
       " 46,\n",
       " 81,\n",
       " 58,\n",
       " 85,\n",
       " 80,\n",
       " 56,\n",
       " 57,\n",
       " 108,\n",
       " 39,\n",
       " 31,\n",
       " 45,\n",
       " 49,\n",
       " 45,\n",
       " 89,\n",
       " 66,\n",
       " 53,\n",
       " 88,\n",
       " 22,\n",
       " 18,\n",
       " 30,\n",
       " 90,\n",
       " 37,\n",
       " 62,\n",
       " 59,\n",
       " 84,\n",
       " 24,\n",
       " 52,\n",
       " 39,\n",
       " 50,\n",
       " 51,\n",
       " 94,\n",
       " 38,\n",
       " 42,\n",
       " 45,\n",
       " 161,\n",
       " 121,\n",
       " 30,\n",
       " 71,\n",
       " 26,\n",
       " 79,\n",
       " 74,\n",
       " 39,\n",
       " 39,\n",
       " 90,\n",
       " 44,\n",
       " 30,\n",
       " 43,\n",
       " 48,\n",
       " 64,\n",
       " 58,\n",
       " 25,\n",
       " 29,\n",
       " 33,\n",
       " 41,\n",
       " 31,\n",
       " 58,\n",
       " 33,\n",
       " 54,\n",
       " 47,\n",
       " 58,\n",
       " 23,\n",
       " 19,\n",
       " 51,\n",
       " 66,\n",
       " 48,\n",
       " 48,\n",
       " 38,\n",
       " 22,\n",
       " 28,\n",
       " 22,\n",
       " 42,\n",
       " 70,\n",
       " 45,\n",
       " 60,\n",
       " 33,\n",
       " 48,\n",
       " 49,\n",
       " 62,\n",
       " 64,\n",
       " 27,\n",
       " 144,\n",
       " 46,\n",
       " 59,\n",
       " 41,\n",
       " 41,\n",
       " 37,\n",
       " 45,\n",
       " 65,\n",
       " 30,\n",
       " 62,\n",
       " 43,\n",
       " 35,\n",
       " 59,\n",
       " 27,\n",
       " 62,\n",
       " 82,\n",
       " 70,\n",
       " 54,\n",
       " 59,\n",
       " 71,\n",
       " 79,\n",
       " 50,\n",
       " 77,\n",
       " 108,\n",
       " 73,\n",
       " 40,\n",
       " 73,\n",
       " 47,\n",
       " 84,\n",
       " 62,\n",
       " 34,\n",
       " 66,\n",
       " 49,\n",
       " 44,\n",
       " 46,\n",
       " 41,\n",
       " 55,\n",
       " 91,\n",
       " 77,\n",
       " 79,\n",
       " 74,\n",
       " 42,\n",
       " 40,\n",
       " 52,\n",
       " 56,\n",
       " 22,\n",
       " 52,\n",
       " 69,\n",
       " 54,\n",
       " 43,\n",
       " 49,\n",
       " 29,\n",
       " 174,\n",
       " 50,\n",
       " 11,\n",
       " 44,\n",
       " 24,\n",
       " 54,\n",
       " 31,\n",
       " 97,\n",
       " 49,\n",
       " 20,\n",
       " 35,\n",
       " 68,\n",
       " 82,\n",
       " 26,\n",
       " 25,\n",
       " 31,\n",
       " 46,\n",
       " 40,\n",
       " 51,\n",
       " 46,\n",
       " 67,\n",
       " 93,\n",
       " 81,\n",
       " 40,\n",
       " 62,\n",
       " 111,\n",
       " 99,\n",
       " 45,\n",
       " 104,\n",
       " 36,\n",
       " 98,\n",
       " 34,\n",
       " 48,\n",
       " 31,\n",
       " 40,\n",
       " 78,\n",
       " 56,\n",
       " 97,\n",
       " 39,\n",
       " 44,\n",
       " 55,\n",
       " 57,\n",
       " 43,\n",
       " 117,\n",
       " 29,\n",
       " 29,\n",
       " 53,\n",
       " 33,\n",
       " 37,\n",
       " 70,\n",
       " 38,\n",
       " 56,\n",
       " 31,\n",
       " 41,\n",
       " 59,\n",
       " 22,\n",
       " 65,\n",
       " 31,\n",
       " 46,\n",
       " 54,\n",
       " 92,\n",
       " 41,\n",
       " 77,\n",
       " 25,\n",
       " 54,\n",
       " 42,\n",
       " 35,\n",
       " 51,\n",
       " 64,\n",
       " 40,\n",
       " 58,\n",
       " 53,\n",
       " 27,\n",
       " 52,\n",
       " 74,\n",
       " 43,\n",
       " 55,\n",
       " 26,\n",
       " 46,\n",
       " 61,\n",
       " 52,\n",
       " 48,\n",
       " 58,\n",
       " 88,\n",
       " 25,\n",
       " 43,\n",
       " 65,\n",
       " 21,\n",
       " 52,\n",
       " 62,\n",
       " 62,\n",
       " 105,\n",
       " 43,\n",
       " 33,\n",
       " 31,\n",
       " 61,\n",
       " 78,\n",
       " 42,\n",
       " 38,\n",
       " 28,\n",
       " 62,\n",
       " 33,\n",
       " 53,\n",
       " 42,\n",
       " 34,\n",
       " 48,\n",
       " 88,\n",
       " 58,\n",
       " 47,\n",
       " 55,\n",
       " 59,\n",
       " 59,\n",
       " 38,\n",
       " 82,\n",
       " 66,\n",
       " 34,\n",
       " 66,\n",
       " 60,\n",
       " 38,\n",
       " 41,\n",
       " 71,\n",
       " 21,\n",
       " 30,\n",
       " 52,\n",
       " 34,\n",
       " 88,\n",
       " 46,\n",
       " 64,\n",
       " 33,\n",
       " 47,\n",
       " 64,\n",
       " 69,\n",
       " 80,\n",
       " 41,\n",
       " 26,\n",
       " 45,\n",
       " 50,\n",
       " 69,\n",
       " 32,\n",
       " 63,\n",
       " 70,\n",
       " 97,\n",
       " 90,\n",
       " 90,\n",
       " 77,\n",
       " 50,\n",
       " 109,\n",
       " 46,\n",
       " 40,\n",
       " 36,\n",
       " 57,\n",
       " 35,\n",
       " 39,\n",
       " 64,\n",
       " 81,\n",
       " 76,\n",
       " 21,\n",
       " 47,\n",
       " 49,\n",
       " 42,\n",
       " 65,\n",
       " 48,\n",
       " 21,\n",
       " 36,\n",
       " 47,\n",
       " 35,\n",
       " 29,\n",
       " 55,\n",
       " 40,\n",
       " 46,\n",
       " 37,\n",
       " 113,\n",
       " 39,\n",
       " 73,\n",
       " 42,\n",
       " 60,\n",
       " 66,\n",
       " 60,\n",
       " 66,\n",
       " 64,\n",
       " 176,\n",
       " 22,\n",
       " 48,\n",
       " 72,\n",
       " 66,\n",
       " 96,\n",
       " 25,\n",
       " 79,\n",
       " 49,\n",
       " 103,\n",
       " 71,\n",
       " 28,\n",
       " 108,\n",
       " 70,\n",
       " 64,\n",
       " 51,\n",
       " 85,\n",
       " 55,\n",
       " 22,\n",
       " 64,\n",
       " 83,\n",
       " 41,\n",
       " 34,\n",
       " 59,\n",
       " 88,\n",
       " 62,\n",
       " 27,\n",
       " 37,\n",
       " 62,\n",
       " 48,\n",
       " 37,\n",
       " 63,\n",
       " 49,\n",
       " 89,\n",
       " 47,\n",
       " 50,\n",
       " 44,\n",
       " 55,\n",
       " 70,\n",
       " 40,\n",
       " 40,\n",
       " 61,\n",
       " 40,\n",
       " 72,\n",
       " 53,\n",
       " 40,\n",
       " 37,\n",
       " 24,\n",
       " 71,\n",
       " 59,\n",
       " 106,\n",
       " 41,\n",
       " 41,\n",
       " 61,\n",
       " 97,\n",
       " 48,\n",
       " 45,\n",
       " 49,\n",
       " 48,\n",
       " 71,\n",
       " 48,\n",
       " 51,\n",
       " 58,\n",
       " 61,\n",
       " 34,\n",
       " 59,\n",
       " 50,\n",
       " 67,\n",
       " 63,\n",
       " 81,\n",
       " 57,\n",
       " 32,\n",
       " 73,\n",
       " 30,\n",
       " 36,\n",
       " 26,\n",
       " 27,\n",
       " 26,\n",
       " 117,\n",
       " 68,\n",
       " 70,\n",
       " 31,\n",
       " 31,\n",
       " 93,\n",
       " 29,\n",
       " 42,\n",
       " 70,\n",
       " 97,\n",
       " 52,\n",
       " 43,\n",
       " 48,\n",
       " 71,\n",
       " 84,\n",
       " 50,\n",
       " 29,\n",
       " 78,\n",
       " 73,\n",
       " 128,\n",
       " 42,\n",
       " 72,\n",
       " 42,\n",
       " 47,\n",
       " 36,\n",
       " 69,\n",
       " 39,\n",
       " 57,\n",
       " 44,\n",
       " 43,\n",
       " 58,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 46,\n",
       " 28,\n",
       " 51,\n",
       " 58,\n",
       " 43,\n",
       " 38,\n",
       " 39,\n",
       " 31,\n",
       " 35,\n",
       " 35,\n",
       " 38,\n",
       " 44,\n",
       " 59,\n",
       " 72,\n",
       " 44,\n",
       " 60,\n",
       " 59,\n",
       " 83,\n",
       " 31,\n",
       " 22,\n",
       " 24,\n",
       " 35,\n",
       " 103,\n",
       " 25,\n",
       " 52,\n",
       " 46,\n",
       " 52,\n",
       " 54,\n",
       " 66,\n",
       " 37,\n",
       " 43,\n",
       " 66,\n",
       " 89,\n",
       " 69,\n",
       " 38,\n",
       " 42,\n",
       " 55,\n",
       " 144,\n",
       " 79,\n",
       " 38,\n",
       " 26,\n",
       " 38,\n",
       " 35,\n",
       " 27,\n",
       " 47,\n",
       " 77,\n",
       " 63,\n",
       " 33,\n",
       " 65,\n",
       " 63,\n",
       " 39,\n",
       " 69,\n",
       " 78,\n",
       " 44,\n",
       " 63,\n",
       " 31,\n",
       " 29,\n",
       " 37,\n",
       " 23,\n",
       " 53,\n",
       " 92,\n",
       " 27,\n",
       " 53,\n",
       " 51,\n",
       " 72,\n",
       " 142,\n",
       " 64,\n",
       " 54,\n",
       " 51,\n",
       " 31,\n",
       " 31,\n",
       " 80,\n",
       " 56,\n",
       " 48,\n",
       " 32,\n",
       " 40,\n",
       " 43,\n",
       " 52,\n",
       " 81,\n",
       " 32,\n",
       " 36,\n",
       " 43,\n",
       " 100,\n",
       " 57,\n",
       " 73,\n",
       " 59,\n",
       " 22,\n",
       " 66,\n",
       " 54,\n",
       " 39,\n",
       " 80,\n",
       " 90,\n",
       " 38,\n",
       " 20,\n",
       " 37,\n",
       " 57,\n",
       " 89,\n",
       " 48,\n",
       " 44,\n",
       " 56,\n",
       " 122,\n",
       " 33,\n",
       " 109,\n",
       " 42,\n",
       " 54,\n",
       " 28,\n",
       " 64,\n",
       " 52,\n",
       " 53,\n",
       " 29,\n",
       " 63,\n",
       " 47,\n",
       " 36,\n",
       " 56,\n",
       " 72,\n",
       " 29,\n",
       " 79,\n",
       " 50,\n",
       " 22,\n",
       " 55,\n",
       " 72,\n",
       " 74,\n",
       " 68,\n",
       " 27,\n",
       " 65,\n",
       " 99,\n",
       " 64,\n",
       " 57,\n",
       " 51,\n",
       " 35,\n",
       " 33,\n",
       " 45,\n",
       " 58,\n",
       " 116,\n",
       " 65,\n",
       " 55,\n",
       " 47,\n",
       " 67,\n",
       " 91,\n",
       " 36,\n",
       " 66,\n",
       " 106,\n",
       " 96,\n",
       " 157,\n",
       " 44,\n",
       " 36,\n",
       " 60,\n",
       " 33,\n",
       " 50,\n",
       " 93,\n",
       " 45,\n",
       " 37,\n",
       " 45,\n",
       " 44,\n",
       " 41,\n",
       " 59,\n",
       " 39,\n",
       " 33,\n",
       " 56,\n",
       " 64,\n",
       " 70,\n",
       " 88,\n",
       " 97,\n",
       " 53,\n",
       " 33,\n",
       " 33,\n",
       " 41,\n",
       " 41,\n",
       " 45,\n",
       " 70,\n",
       " 39,\n",
       " 48,\n",
       " 69,\n",
       " 55,\n",
       " 28,\n",
       " 49,\n",
       " 62,\n",
       " 53,\n",
       " 52,\n",
       " 40,\n",
       " 47,\n",
       " 26,\n",
       " 63,\n",
       " 45,\n",
       " 44,\n",
       " 81,\n",
       " 52,\n",
       " 68,\n",
       " 43,\n",
       " 41,\n",
       " 39,\n",
       " 41,\n",
       " 44,\n",
       " 91,\n",
       " 30,\n",
       " 28,\n",
       " 42,\n",
       " 52,\n",
       " 39,\n",
       " 38,\n",
       " 60,\n",
       " 78,\n",
       " 25,\n",
       " 79,\n",
       " 93,\n",
       " 59,\n",
       " 46,\n",
       " 49,\n",
       " 22,\n",
       " 74,\n",
       " 103,\n",
       " 60,\n",
       " 63,\n",
       " 83,\n",
       " 133,\n",
       " 84,\n",
       " 28,\n",
       " 54,\n",
       " 26,\n",
       " 52,\n",
       " 33,\n",
       " 66,\n",
       " 59,\n",
       " 101,\n",
       " 37,\n",
       " 36,\n",
       " 35,\n",
       " 29,\n",
       " 72,\n",
       " 43,\n",
       " 29,\n",
       " 62,\n",
       " 70,\n",
       " 38,\n",
       " 26,\n",
       " 134,\n",
       " 41,\n",
       " 28,\n",
       " 28,\n",
       " 25,\n",
       " 48,\n",
       " 49,\n",
       " 30,\n",
       " 40,\n",
       " 57,\n",
       " 69,\n",
       " 103,\n",
       " 46,\n",
       " 46,\n",
       " 37,\n",
       " 82,\n",
       " 38,\n",
       " 65,\n",
       " 44,\n",
       " 100,\n",
       " 42,\n",
       " 61,\n",
       " 106,\n",
       " 81,\n",
       " 44,\n",
       " 66,\n",
       " 59,\n",
       " 51,\n",
       " 34,\n",
       " 34,\n",
       " 68,\n",
       " 31,\n",
       " 57,\n",
       " 43,\n",
       " 48,\n",
       " 31,\n",
       " 76,\n",
       " 43,\n",
       " 38,\n",
       " 74,\n",
       " 119,\n",
       " 43,\n",
       " 49,\n",
       " 53,\n",
       " 49,\n",
       " 52,\n",
       " 43,\n",
       " 64,\n",
       " 79,\n",
       " 70,\n",
       " 63,\n",
       " 76,\n",
       " 62,\n",
       " 49,\n",
       " 46,\n",
       " 39,\n",
       " 62,\n",
       " 27,\n",
       " 84,\n",
       " 30,\n",
       " 74,\n",
       " 43,\n",
       " 119,\n",
       " 52,\n",
       " 107,\n",
       " 34,\n",
       " 30,\n",
       " 58,\n",
       " 27,\n",
       " 50,\n",
       " 80,\n",
       " 25,\n",
       " 49,\n",
       " 57,\n",
       " 34,\n",
       " 56,\n",
       " 66,\n",
       " 36,\n",
       " 53,\n",
       " 51,\n",
       " 80,\n",
       " 39,\n",
       " 60,\n",
       " 66,\n",
       " 41,\n",
       " 44,\n",
       " 46,\n",
       " 44,\n",
       " 36,\n",
       " 55,\n",
       " 108,\n",
       " 46,\n",
       " 63,\n",
       " 51,\n",
       " 32,\n",
       " 85,\n",
       " 49,\n",
       " 44,\n",
       " 38,\n",
       " 72,\n",
       " 55,\n",
       " 31,\n",
       " 50,\n",
       " 84,\n",
       " 38,\n",
       " 49,\n",
       " 50,\n",
       " 27,\n",
       " 36,\n",
       " 40,\n",
       " 67,\n",
       " 34,\n",
       " 27,\n",
       " 22,\n",
       " 34,\n",
       " 40,\n",
       " 35,\n",
       " 85,\n",
       " 59,\n",
       " 66,\n",
       " 82,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng = []\n",
    "for s in train_dataset['sentence']:\n",
    "    leng.append(len(tokenizer.tokenize(s)))\n",
    "leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8182440\n"
     ]
    }
   ],
   "source": [
    "print(len(entity_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['호남', '##이', '기반', '##인', '바른', '##미', '##래', '##당', '·', '<O:ORG>', '대안', '##신', '##당', '</O:ORG>', '·', '<S:ORG>', '민주', '##평', '##화', '##당', '</S:ORG>', '이', '우여곡절', '끝', '##에', '합당', '##해', '민생', '##당', '(', '가칭', ')', '으로', '재', '##탄', '##생', '##한다', '.']\n",
      "[9, 15]\n",
      "[13, 20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(train_dataset['sentence'][1])\n",
    "tmp = [0 for _ in range(256)]\n",
    "tmpidx = [[],[]]\n",
    "print(tokens)\n",
    "for i in range(len(tokens)):\n",
    "    if '<S:' in tokens[i] or '<O:' in tokens[i]:\n",
    "        # print(tokens[i])\n",
    "        tmpidx[0].append(i)\n",
    "    elif '</S:' in tokens[i] or '</O:' in tokens[i]:\n",
    "        # print(tokens[i])\n",
    "        tmpidx[1].append(i)\n",
    "        \n",
    "def update_ranges_to_1(start_tokens, end_tokens, maxlen=251):\n",
    "    print(start_tokens)\n",
    "    print(end_tokens)\n",
    "    res = []\n",
    "    res += [0] * (start_tokens[0]+1) + [1] * (end_tokens[0]-start_tokens[0]-1)\\\n",
    "    + [0] * (start_tokens[1]-end_tokens[0]+1)\\\n",
    "    + [1] * (end_tokens[1]-start_tokens[1]-1)\\\n",
    "    + [0] * (maxlen-end_tokens[1])\n",
    "    return res\n",
    "\n",
    "len(update_ranges_to_1(tmpidx[0], tmpidx[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train['entity_ids'] = entity_ids\n",
    "tokenized_train.keys()\n",
    "# 'input_ids', 'token_type_ids', 'attention_mask', 'entity_ids'\n",
    "type(tokenized_train['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train['entity_ids'][1])\n",
    "len(tokenized_train['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "  num_label = []\n",
    "  with open('dict_label_to_num.pkl', 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "\n",
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'sentence': '〈Something〉는 <O:PER> 조지 해리슨 </O:PER>이 쓰고 <S:ORG> 비틀즈 </S:ORG>가 1969년 앨범 《Abbey Road》에 담은 노래다.',\n",
       " 'subject_entity': '비틀즈',\n",
       " 'object_entity': '조지 해리슨',\n",
       " 'label': 'no_relation'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item = {key: val[1].clone().detach() for key, val in train_dataset.items()}\n",
    "item = {key: val[0] for key, val in train_dataset.items()}\n",
    "item\n",
    "# RE_train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_entity = []\n",
    "i = 0\n",
    "for e01, e02 in zip(train_dataset['subject_entity'], train_dataset['object_entity']):\n",
    "    # Prompting Sentence \n",
    "    if i == 5 : break\n",
    "    temp = ''\n",
    "    temp = e01 + '와(과) ' + e02 + '의 관계' #+'[SEP]'\n",
    "    temp = tokenizer(temp, add_special_tokens=True)#, add_special_tokens=True)\n",
    "    concat_entity.append(temp)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [0, 29830, 2522, 12, 604, 13, 8373, 14113, 2234, 2079, 3654, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [0, 3772, 2139, 2267, 2481, 2522, 12, 604, 13, 5605, 2250, 2481, 2079, 3654, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [0, 4104, 10904, 2522, 12, 604, 13, 3629, 17287, 20212, 2079, 3654, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [0, 27930, 24393, 2024, 2522, 12, 604, 13, 6580, 2144, 2079, 3654, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [0, 20289, 20562, 2522, 12, 604, 13, 14925, 2079, 3654, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_ids': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entity_embedding(concat_entity[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m subj_start_id \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<S:PER>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<S:ORG>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m subj_end_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</S:PER>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</S:ORG>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m obj_start_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<O:PER>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<O:ORG>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<O:LOC>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<O:DAT>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<O:POH>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<O:NOH>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "subj_start_id = tokenizer.convert_tokens_to_ids([\"<S:PER>\", \"<S:ORG>\"])\n",
    "subj_end_id = tokenizer.convert_tokens_to_ids([\"</S:PER>\", \"</S:ORG>\"])\n",
    "obj_start_id = tokenizer.convert_tokens_to_ids([\"<O:PER>\", \"<O:ORG>\", \"<O:LOC>\", \"<O:DAT>\", \"<O:POH>\", \"<O:NOH>\"])\n",
    "obj_end_id = tokenizer.convert_tokens_to_ids([\"</O:PER>\", \"</O:ORG>\", \"</O:LOC>\", \"</O:DAT>\", \"</O:POH>\", \"</O:NOH>\"])\n",
    "obj_start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def update_ranges_to_1(start_tokens, end_tokens, length):\n",
    "    res = np.zeros(length, dtype=int)\n",
    "    for start, end in zip(start_tokens, end_tokens):\n",
    "        res[start + 1:end] = 1\n",
    "    return res\n",
    "\n",
    "def entity_ids_maker(data, start_id, end_id):\n",
    "    entity_ids = []\n",
    "    \n",
    "    for ids in tqdm(data):\n",
    "        length = len(ids)\n",
    "        startidx = [i for i, id in enumerate(ids) if id in start_id]\n",
    "        endidx = [i for i, id in enumerate(ids) if id in end_id]\n",
    "\n",
    "        if startidx and endidx:\n",
    "            tmp = update_ranges_to_1(startidx, endidx, length)\n",
    "            entity_ids.append(tmp)\n",
    "    \n",
    "    entity_ids = torch.tensor(entity_ids, dtype=torch.int)\n",
    "    return entity_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_position_embedding(tokenizer, input_ids):\n",
    "  special_token2id = {k:v for k,v in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)}\n",
    "\n",
    "  sub_token_id = special_token2id['@']\n",
    "  obj_token_id = special_token2id['#']\n",
    "  \n",
    "  pos_embeddings = []\n",
    "\n",
    "  for y in tqdm(input_ids):\n",
    "    pos = []\n",
    "    for j in range(0, len(y)):\n",
    "      if len(pos) == 4:\n",
    "        break\n",
    "      if y[j] == sub_token_id:\n",
    "        pos.append(j)\n",
    "\n",
    "      if y[j] == obj_token_id:\n",
    "        pos.append(j)\n",
    "    pos_embeddings.append(pos)\n",
    "  return pos_embeddings\n",
    "  # return torch.tensor(pos_embeddings, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '[CLS] 〈 Something 〉 는 # * PER * 조지 해리슨 # 이 쓰고 @ * ORG * 비틀즈 @ 가 1969년 앨범 《 Abbey Road 》 에 담은 노래다. [SEP] 비틀즈와 ( 과 ) 조지 해리슨은 ( 는 )? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32470/32470 [00:13<00:00, 2359.66it/s]\n"
     ]
    }
   ],
   "source": [
    "ent_pos_emb = get_entity_position_embedding(tokenizer, tokenized_train['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ent_pos_emb:\n",
    "    if len(i) == 4:continue\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_entity_pos_emb(pos_emb):\n",
    "    ddd = []\n",
    "    for idx, ids in tqdm(enumerate(pos_emb)):\n",
    "        ent_emb = []\n",
    "        ent_emb += [0] * ids[0] +\\\n",
    "                   [1] * (ids[1] - ids[0] + 1) + \\\n",
    "                   [0] * (ids[2] - ids[1]-1) + \\\n",
    "                   [1] * (ids[3] - ids[2] + 1) + \\\n",
    "                   [0] * (256 - ids[3]-1)\n",
    "        ddd.append(ent_emb)\n",
    "    return torch.Tensor(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32470it [00:00, 130683.86it/s]\n"
     ]
    }
   ],
   "source": [
    "asdf = making_entity_pos_emb(ent_pos_emb)\n",
    "# asdf = torch.Tensor(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf\n",
    "# len(asdf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32470, 256])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train['entity_ids'] = asdf\n",
    "tokenized_train.keys()\n",
    "tokenized_train['input_ids'].shape\n",
    "tokenized_train['entity_ids'].shape # same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
