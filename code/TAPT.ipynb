{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 22.5k/22.5k [00:00<00:00, 11.1MB/s]\n",
      "Downloading data: 100%|██████████| 6.65M/6.65M [00:00<00:00, 18.1MB/s]\n",
      "Downloading data: 100%|██████████| 1.54M/1.54M [00:00<00:00, 6.67MB/s]\n",
      "Generating train split: 100%|██████████| 32470/32470 [00:00<00:00, 449319.04 examples/s]\n",
      "Generating validation split: 100%|██████████| 7765/7765 [00:00<00:00, 416597.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "klue_re_dataset_train = load_dataset(\"klue\", \"re\", split=\"train\")\n",
    "klue_re_dataset_val = load_dataset(\"klue\", \"re\", split=\"validation\")\n",
    "\n",
    "klue_re_dataset_train = klue_re_dataset_train['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey Road》에 담은 노래다.\n"
     ]
    }
   ],
   "source": [
    "print(klue_re_dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "klue_re_dataset_val= klue_re_dataset_val['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer , AutoModelForMaskedLM, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = klue_re_dataset_train\n",
    "val_data = klue_re_dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"20대 남성 A(26)씨가 아버지 치료비를 위해 B(30)씨가 모아둔 돈을 훔쳐 인터넷 방송 BJ에게 '별풍선'으로 쏜 사실이 알려졌다.\",\n",
       " '그러나 심 의원은 보좌진이 접속 권한을 받아 정부 업무추진비 사용 내역 등을 다운받았음에도 정부가 허위 사실을 유포하는 등 국정감사 활동을 방해하고 있다고 반박했고, 김동연 경제부총리 겸 기획재정부 장관과 김재훈 재정정보원장, 기재부 관계자 등을 무고 등 혐의로 전날 맞고발했다.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self,tokenizer,data,block_size):\n",
    "        encoded_data = tokenizer(data,\n",
    "                  truncation=True,\n",
    "                  max_length=block_size)\n",
    "        self.examples = encoded_data['input_ids']\n",
    "        self.examples = [{\"input_ids\": torch.tensor(ex,dtype=torch.long)} for ex in self.examples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_pretraining(tokenizer,train_input,val_input):\n",
    "    train_dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data=train_input,\n",
    "        block_size=512,\n",
    "    )\n",
    "    # set mlm task\n",
    "    # DataCollatorForSOP로 변경시 SOP 사용 가능 (DataCollatorForLanguageModeling)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15 # 0.3\n",
    "    )\n",
    "    eval_dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data=val_input,\n",
    "        block_size=512,\n",
    "    )\n",
    "\n",
    "    return train_dataset, data_collator, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainer_for_pretraining(\n",
    "        model,\n",
    "        data_collator,\n",
    "        dataset,\n",
    "        eval_dataset,\n",
    "        epoch = 10,\n",
    "        batch_size = 16,\n",
    "        accumalation_step = 1,):\n",
    "\n",
    "     # set training args\n",
    "    training_args = TrainingArguments(\n",
    "        report_to = 'tensorboard',\n",
    "        output_dir='./pretraining_outputs',\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epoch,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=accumalation_step,\n",
    "        evaluation_strategy = 'steps',\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        seed=42,\n",
    "        save_strategy='steps'\n",
    "        # evaluation_strategy='epoch',\n",
    "    )\n",
    "\n",
    "\n",
    "    # set Trainer class for pre-training\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3,early_stopping_threshold=0.001)]\n",
    "\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain():\n",
    "    \"\"\"MLM task 기반 사전학습 진행\"\"\"\n",
    "    # fix a seed\n",
    "    pl.seed_everything(seed=42)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # set model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\")\n",
    "    model.to(device)\n",
    "\n",
    "    # set data\n",
    "    train_dataset, data_collator, eval_dataset = prepare_dataset_for_pretraining(tokenizer, train_data, val_data)\n",
    "\n",
    "    # set trainer\n",
    "    trainer = set_trainer_for_pretraining(model,data_collator,train_dataset,eval_dataset)\n",
    "\n",
    "    # train model\n",
    "    print(\"--- Start train ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Finish train ---\")\n",
    "    model.save_pretrained(\"./pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9dde47e60c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrain' is not defined"
     ]
    }
   ],
   "source": [
    "pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_MARKERS = dict(\n",
    "    subject_start_per_marker=\"<S:PER>\",\n",
    "    subject_start_org_marker=\"<S:ORG>\",\n",
    "    subject_start_loc_marker=\"<S:LOC>\",\n",
    "    subject_end_per_marker =\"</S:PER>\",\n",
    "    subject_end_org_marker =\"</S:ORG>\",\n",
    "    subject_end_loc_marker=\"</S:LOC>\",\n",
    "    object_start_per_marker=\"<O:PER>\",\n",
    "    object_start_org_marker=\"<O:ORG>\",\n",
    "    object_start_loc_marker=\"<O:LOC>\",\n",
    "    object_start_dat_marker=\"<O:DAT>\",\n",
    "    object_start_poh_marker=\"<O:POH>\",\n",
    "    object_start_noh_marker=\"<O:NOH>\",\n",
    "    object_end_per_marker =\"</O:PER>\",\n",
    "    object_end_org_marker =\"</O:ORG>\",\n",
    "    object_end_loc_marker =\"</O:LOC>\",\n",
    "    object_end_dat_marker =\"</O:DAT>\",\n",
    "    object_end_poh_marker =\"</O:POH>\",\n",
    "    object_end_noh_marker =\"</O:NOH>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, EarlyStoppingCallback\n",
    "from transformers import AutoModelForMaskedLM, AutoModel\n",
    "from load_data import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from traindevsplit import * # train_dev_split\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "\n",
    "new_special_tokens = list(TYPE_MARKERS.values())\n",
    "\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': new_special_tokens})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./pretrained_roberta_large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./pretrained_roberta_large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32018, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "      # Setting model hyperparameter\n",
    "      model_path = './pretrained_roberta_large'\n",
    "      model_config = AutoConfig.from_pretrained(f'{model_path}/config.json')\n",
    "      model_config.num_labels = 30\n",
    "\n",
    "      # Load the sequence classification model\n",
    "      model = AutoModelForSequenceClassification.from_pretrained(model_path, config=model_config)\n",
    "      model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ranges_to_1(start_tokens, end_tokens, length):\n",
    "    res = np.zeros(length, dtype=int)\n",
    "    for start, end in zip(start_tokens, end_tokens):\n",
    "        res[start + 1:end] = 1\n",
    "    return res\n",
    "\n",
    "def entity_ids_maker(data, start_id, end_id):\n",
    "    entity_ids = []\n",
    "    \n",
    "    for ids in tqdm(data):\n",
    "        length = len(ids)\n",
    "        startidx = [i for i, id in enumerate(ids) if id in start_id]\n",
    "        endidx = [i for i, id in enumerate(ids) if id in end_id]\n",
    "\n",
    "        if startidx and endidx:\n",
    "            tmp = update_ranges_to_1(startidx, endidx, length)\n",
    "            entity_ids.append(tmp)\n",
    "    \n",
    "    entity_ids = torch.tensor(entity_ids, dtype=torch.int)\n",
    "    return entity_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_ids_maker(data, start_id, end_id): # data에는 tokenizer를 거쳐 나온 input_ids가 들어온다\n",
    "    def update_ranges_to_1(start_tokens, end_tokens, maxlen=251):\n",
    "        res = np.zeros(maxlen, dtype=int    )\n",
    "        for start_token, end_token in zip(start_tokens, end_tokens):\n",
    "            res[start_token + 1:end_token] = 1\n",
    "        return res\n",
    "    \n",
    "             \n",
    "    entity_ids = []\n",
    "    for ids in tqdm(data):\n",
    "        length = len(ids)\n",
    "\n",
    "        startidx = [i for i, id in enumerate(ids) if id in start_id]\n",
    "        endidx = [i for i, id in enumerate(ids) if id in end_id]\n",
    "\n",
    "        if startidx and endidx:\n",
    "            tmp = update_ranges_to_1(startidx, endidx, maxlen=length)\n",
    "            entity_ids.append(tmp)\n",
    "\n",
    "                \n",
    "    entity_ids = torch.Tensor(entity_ids, dtype=torch.int)\n",
    "    return entity_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing dataset\n",
    "    tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "    train_ent_pos_emb = get_entity_position_embedding(tokenizer, tokenized_train['input_ids'])\n",
    "    # print(len(tokenized_train['input_ids'][0]))\n",
    "    # for i in train_ent_pos_emb:\n",
    "    #     if len(i) == 4:continue\n",
    "    #     else:\n",
    "    #         print(\"error\")\n",
    "    tokenized_train['entity_ids'] = making_entity_pos_emb(train_ent_pos_emb)\n",
    "    # entity_ids = entity_ids_maker(train_dataset, tokenizer)\n",
    "    # tokenized_train['entity_ids'] = entity_ids\n",
    "    \n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "    dev_ent_pos_emb = get_entity_position_embedding(tokenizer, tokenized_dev['input_ids'])\n",
    "    tokenized_dev['entity_ids'] = making_entity_pos_emb(dev_ent_pos_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
